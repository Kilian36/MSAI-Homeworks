{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(loss, grad_l, w0, data, batch_size, n_epochs, alpha = 0.01):\n",
    "    ''' \n",
    "    Input:\n",
    "        l: the function l(w; D) we want to optimize.\n",
    "        It is supposed to be a Python function, not an array.\n",
    "        grad_l: the gradient of l(w; D). It is supposed to be a Python function, not an array.\n",
    "        w0: an n-dimensional array which represents the initial iterate. By default, it\n",
    "        should be randomly sampled.\n",
    "        data: a tuple (x, y) that contains the two arrays x and y, where x is the input data,\n",
    "        y is the output data.\n",
    "        batch_size: an integer. The dimension of each batch. Should be a divisor of the number of data.\n",
    "        n_epochs: an integer. The number of epochs you want to reapeat the iterations.\n",
    "    Output:\n",
    "        w: an array that contains the value of w_k FOR EACH iterate w_k (not only the latter).\n",
    "        f_val: an array that contains the value of l(w_k; D)\n",
    "        FOR EACH iterate w_k ONLY after each epoch.\n",
    "        grads: an array that contains the value of grad_l(w_k; D)\n",
    "        FOR EACH iterate w_k ONLY after each epoch.\n",
    "        err: an array the contains the value of ||grad_l(w_k; D)||_2\n",
    "        FOR EACH iterate w_k ONLY after each epoch.\n",
    "    '''\n",
    "\n",
    "    epoch = 1\n",
    "\n",
    "\n",
    "    # divide data in X and y\n",
    "    X, y = data\n",
    "    l, N = X.shape\n",
    "\n",
    "    total_number_iter = (N // batch_size) * n_epochs\n",
    "    tot_iter = 0\n",
    "    \n",
    "    # Every row is a wieght vector\n",
    "    weights_array = np.zeros((total_number_iter, l))\n",
    "    loss_array = np.zeros((n_epochs, 1))\n",
    "    gradient_array = np.zeros((n_epochs, l)) \n",
    "    err_array = np.zeros((n_epochs, 1))\n",
    "\n",
    "    iter_epoch = N // batch_size\n",
    "    # Now lest shuffle the dataset\n",
    "    X_shuffled, y_shuffled = shuffle_dataset(X, y, N // batch_size)\n",
    "    \n",
    "    while epoch <= n_epochs:\n",
    "        # Define the indexes of the batch\n",
    "        start = 0 \n",
    "        end = batch_size\n",
    "        for i in range(iter_epoch):\n",
    "            # Add the actual weights to the array\n",
    "            weights_array[tot_iter, :] = np.reshape(w0, (l, ))\n",
    "            \n",
    "            # Selecte the sample\n",
    "            X_sample = X_shuffled[:, start:end]\n",
    "            y_sample = y_shuffled[start:end]\n",
    "            \n",
    "            # update the batch indexes\n",
    "            start = end\n",
    "            end = end + batch_size\n",
    "\n",
    "            # Compute the gradient\n",
    "            grad = grad_loss(w0, X_sample, y_sample)\n",
    "            \n",
    "            # Update the weights\n",
    "            w0 -= alpha * np.reshape(grad, (l, 1))\n",
    "            tot_iter += 1\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                loss_value = loss_func(w0, X_sample, y_sample)\n",
    "                grad_norm = np.linalg.norm(grad)\n",
    "                print('At iteration {}\\ntotal loss = {} ; error = {}'.format(tot_iter, loss_value, grad_norm))\n",
    "        \n",
    "        # Compute the loss value\n",
    "        loss_value = loss_func(w0, X_sample, y_sample)\n",
    "\n",
    "        # Compute the norm of the gradient\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        loss_array[epoch - 1] = np.reshape(loss_value, 1)\n",
    "        gradient_array[epoch - 1, :] = np.reshape(grad, (l, ))\n",
    "        err_array[epoch - 1] = grad_norm\n",
    " \n",
    "        X_shuffled, y_shuffled = shuffle_dataset(X, y, N // batch_size)\n",
    "        epoch += 1\n",
    "\n",
    "    return weights_array, loss_array, gradient_array, err_array\n",
    "           \n",
    "\n",
    "\n",
    "def shuffle_dataset(X, y, batch_size):\n",
    "    '''\n",
    "    Input : X is the dataset\n",
    "            y is the labels array\n",
    "            batch_size is the lenght of one subset\n",
    "    Return : X and y shuffled randomly\n",
    "            \n",
    "    '''\n",
    "    idxs = np.arange(X.shape[1])\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "    X_sample = X[:, idxs]\n",
    "    y_sample = y[idxs]\n",
    "    \n",
    "    return X_sample, y_sample \n",
    "\n",
    "\n",
    "\n",
    "def loss_func(w0, x_hat, y):\n",
    "    '''     \n",
    "    Input : w0 is the column of weights with shape = l+1 x 1. \n",
    "                (the weights are repeated to vectorize the operation).\n",
    "            x_hat is a matrix vector containing the batch. shape = l+1 x batch_size. \n",
    "            y is the label vector.\n",
    "    Returns : the MSE of the prodiction xhat @ wo \n",
    "    '''\n",
    "    _, N = x_hat.shape\n",
    "\n",
    "\n",
    "    z = x_hat.T @ w0\n",
    "    #print('Dot product : {}'.format(z))\n",
    "    fw = sigmoid(z)\n",
    "    #print('Objective val :  {} Shape : {}'.format(fw, fw.shape))\n",
    "    y = np.reshape(y, (N, 1))\n",
    "\n",
    "    err = fw - y\n",
    "    #print('Error : {} Error shape : {}'.format(err, err.shape))\n",
    "    MSE = np.linalg.norm((err), axis = 1)**2\n",
    "    #print('MSE : {}'.format(MSE))\n",
    "    return (np.sum(MSE)) / N\n",
    "\n",
    "\n",
    "def grad_loss(w0, x_hat, y):\n",
    "    '''      \n",
    "    Input : w0 is a column vector of weights with shape = l+1 x 1\n",
    "            x_hat is a matrix with the data-points of my batch (l + 1 x N) \n",
    "            y is the label vector\n",
    "    Returns : the gradient of the loss function computed over w0 for \n",
    "              the whole batch\n",
    "    '''\n",
    "    _, N = x_hat.shape\n",
    "\n",
    "    z = x_hat.T @ w0\n",
    "\n",
    "    fw = sigmoid(z) # I obtain a row vector\n",
    "    \n",
    "    # I need column vectors\n",
    "    y = np.reshape(y, (N, 1))\n",
    "    fw = np.reshape(fw, (N, 1))\n",
    "\n",
    "    #print('obj : {} dot : {}'.format(fw, z))\n",
    "    grad_matrix =  fw * (1 - fw) * x_hat.T * (fw - y)\n",
    "    #print('Grad matrix shape : {}'.format(grad_matrix))\n",
    "    # I am interested on the sum over the batch.\n",
    "    \n",
    "    return (np.sum(grad_matrix, axis = 0)) / N\n",
    "\n",
    "# Forward step implementation\n",
    "def predictor(X_hat, w, treshold = 0.5):\n",
    "    '''     \n",
    "    Input : X_hat --> test dataset with a one on the first row (shape = l+1 x N)\n",
    "            w --> weight vector fitted\n",
    "            treshold --> probability treshold to validate a prediciton (default 0.5)\n",
    "    Returns : an array with the predictions\n",
    "    '''\n",
    "    _, N = X_hat.shape\n",
    "    fw = sigmoid(X_hat.T @ w)\n",
    "    preds = np.zeros((N,))\n",
    "    preds = fw > treshold\n",
    "    return preds\n",
    "\n",
    "def accuracy(preds, labels) -> float:\n",
    "    ''' \n",
    "    Input : preds --> a vector with the predictions made by the modeÃ²\n",
    "            labels --> a vector with the ground truth of the test set\n",
    "    Returns : the sum over the array made by the elements for which the result is the same\n",
    "              divided by N (number of elements)\n",
    "    '''\n",
    "    N = preds.shape\n",
    "    xor_arr = np.bitwise_xor(preds, labels)\n",
    "    nxor_arr = np.logical_not(xor_arr) \n",
    "    return np.sum(nxor_arr) / N\n",
    "    \n",
    "\n",
    "def sigmoid(z):\n",
    "    '''         \n",
    "    Input : float value\n",
    "    Returns : sigmoid of z\n",
    "    '''\n",
    "    return 1 / ( 1 + np.exp(-z))\n",
    "\n",
    "\n",
    "\n",
    "def divide_dataset(X, Y, N_train):\n",
    "    '''\n",
    "    Input: X, y dataset and labels\n",
    "           N_train is the number of element of the train set\n",
    "    Returns: The dasatet randomly divided into train and test\n",
    "             according to N_train in the form :\n",
    "             (Xtrain, Ytrain), (Xtest, Ytest) \n",
    "    '''\n",
    "    \n",
    "    idxs = np.arange(0, len(X[0])-1, 1)\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    trian_idxs = idxs[:N_train]\n",
    "    test_idxs = idxs[N_train:]\n",
    "\n",
    "\n",
    "    XTrain = X[:,trian_idxs]\n",
    "    YTrain = Y[trian_idxs]\n",
    "\n",
    "    XTest = X[:,test_idxs]\n",
    "    YTest = Y[test_idxs]\n",
    "\n",
    "    return (XTrain, YTrain), (XTest, YTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_digits(num):\n",
    "    i = 0\n",
    "    digits = []\n",
    "    while i < num:\n",
    "        digit = input(\"Insert a digit : \")\n",
    "        try:\n",
    "            digits.append(int(digit)) \n",
    "            i += 1\n",
    "        except:\n",
    "            print(\"Not an integer\")\n",
    "    return digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of logistic regression\n",
    "The implementation of binary logistic regression on the MNIST dataset.\n",
    "It's possible to choose the digits for the user.\n",
    "#### 1) Dataset opening and division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 9]\n",
      "Single class datasets shape\n",
      "[(784, 4177), (784, 4188)]\n",
      "Dataset with only selected digits\n",
      "X shape : (784, 8365)\n",
      "Y shape : (8365,) \n",
      "Splitting into train and test\n",
      "1\n",
      "Train set shape : (784, 836)\n",
      "Test set shape : (784, 7528)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('data.csv')\n",
    "data_arr = np.array(data)\n",
    "\n",
    "X = data_arr[:, :-1].T\n",
    "y = data_arr[:, 0]\n",
    "\n",
    "\n",
    "classes = get_digits(2)\n",
    "print(classes)\n",
    "\n",
    "\n",
    "# I apply my mask to the two arrays\n",
    "masks = [y == i for i in classes]\n",
    "\n",
    "# List of dataset with a unique digit taken from mask\n",
    "Xlist = [X[:, mask] for mask in masks]\n",
    "Ylist = [y[mask] for mask in masks]\n",
    "\n",
    "print('Single class datasets shape')\n",
    "print([X.shape for X in Xlist])\n",
    "\n",
    "# Lest put them into a single nunpy array\n",
    "X = np.concatenate(Xlist, axis=1) \n",
    "y = np.concatenate(Ylist)\n",
    "\n",
    "# Binary conversion \n",
    "# the labels must be binary\n",
    "y[y == classes[0]] = 0\n",
    "y[y == classes[1]] = 1\n",
    "\n",
    "l, N = X.shape\n",
    "\n",
    "print('Dataset with only selected digits')\n",
    "print('X shape : {}\\nY shape : {} '.format(X.shape, y.shape))\n",
    "\n",
    "print(\"Splitting into train and test\")\n",
    "(Xtrain, Ytrain), (Xtest, Ytest) = divide_dataset(X, y, int(0.1 * N))\n",
    "print(max(Ytrain))\n",
    "print('Train set shape : {}'.format(Xtrain.shape))\n",
    "print('Test set shape : {}'.format(Xtest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2) Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 1\n",
      "total loss = 8.408414808756972e-10 ; error = 56.15761645106224\n",
      "At iteration 101\n",
      "total loss = 8.848149848907279e-16 ; error = 1.5364963979693884e-12\n",
      "At iteration 201\n",
      "total loss = 0.0 ; error = 294.3041622803303\n",
      "At iteration 301\n",
      "total loss = 3.612484732401154e-09 ; error = 7.08987507108312e-06\n",
      "At iteration 401\n",
      "total loss = 3.652748781629295e-91 ; error = 9.523869835275436e-88\n",
      "At iteration 501\n",
      "total loss = 1.8568033618267466e-29 ; error = 4.971166831055746e-26\n",
      "At iteration 601\n",
      "total loss = 2.206821509448512e-50 ; error = 4.421370060943043e-47\n",
      "At iteration 701\n",
      "total loss = 0.999627368312386 ; error = 0.3197087737447451\n",
      "At iteration 801\n",
      "total loss = 1.2182552572481185e-13 ; error = 2.848128423666277e-10\n"
     ]
    }
   ],
   "source": [
    "l, N = Xtrain.shape\n",
    "# Computation of the dataset with ones in the first element\n",
    "X_hat = np.concatenate((np.ones((1, N)), Xtrain), axis = 0)\n",
    "\n",
    "# Starting weights vector\n",
    "w0 = np.random.normal(0, 0.001, (l + 1, 1))\n",
    "\n",
    "# Dataset tuple\n",
    "data = (X_hat, Ytrain)\n",
    "\n",
    "# Batch size, it could be any number less than N\n",
    "batch_size = 1 \n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Learning rate\n",
    "lr = 1e-4\n",
    "\n",
    "# Training phase\n",
    "w, loss, grad, err = logistic_regression(loss_func, grad_loss, w0, data, batch_size, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Testing the accuracy of the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 92.867%\n"
     ]
    }
   ],
   "source": [
    "# Lets compute Xtest with ones on the first row\n",
    "Xtesthat = np.concatenate((np.ones((1, Xtest.shape[1])), Xtest), axis = 0)\n",
    "\n",
    "# Select the final weights from the list\n",
    "final_weights = w[-1]\n",
    "\n",
    "# Select the treshold for which a prediction is considered good\n",
    "treshold = 0.5\n",
    "\n",
    "#Lets now run the predictor  \n",
    "preds = predictor(Xtesthat, final_weights, treshold)\n",
    "\n",
    "res = float(accuracy(preds, Ytest) * 100) # to get the precentage\n",
    "print('Accuracy : {}%'.format(round(res, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "047dab9871d20290a7bb518a08f15d50efb26882ff70546ff4b30ef4aaeabb7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
